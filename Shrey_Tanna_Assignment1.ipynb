{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Name: SHREY TANNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           0\n",
      "diagnosis                    0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "Unnamed: 32                569\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 32','id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'].replace(['B','M'],[1,-1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         -1        17.99         10.38          122.80     1001.0   \n",
       "1         -1        20.57         17.77          132.90     1326.0   \n",
       "2         -1        19.69         21.25          130.00     1203.0   \n",
       "3         -1        11.42         20.38           77.58      386.1   \n",
       "4         -1        20.29         14.34          135.10     1297.0   \n",
       "5         -1        12.45         15.70           82.57      477.1   \n",
       "6         -1        18.25         19.98          119.60     1040.0   \n",
       "7         -1        13.71         20.83           90.20      577.9   \n",
       "8         -1        13.00         21.82           87.50      519.8   \n",
       "9         -1        12.46         24.04           83.97      475.9   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760         0.30010              0.14710   \n",
       "1          0.08474           0.07864         0.08690              0.07017   \n",
       "2          0.10960           0.15990         0.19740              0.12790   \n",
       "3          0.14250           0.28390         0.24140              0.10520   \n",
       "4          0.10030           0.13280         0.19800              0.10430   \n",
       "5          0.12780           0.17000         0.15780              0.08089   \n",
       "6          0.09463           0.10900         0.11270              0.07400   \n",
       "7          0.11890           0.16450         0.09366              0.05985   \n",
       "8          0.12730           0.19320         0.18590              0.09353   \n",
       "9          0.11860           0.23960         0.22730              0.08543   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "5         0.2087  ...         15.47          23.75           103.40   \n",
       "6         0.1794  ...         22.88          27.66           153.20   \n",
       "7         0.2196  ...         17.06          28.14           110.60   \n",
       "8         0.2350  ...         15.49          30.73           106.20   \n",
       "9         0.2030  ...         15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "5       741.6            0.1791             0.5249           0.5355   \n",
       "6      1606.0            0.1442             0.2576           0.3784   \n",
       "7       897.0            0.1654             0.3682           0.2678   \n",
       "8       739.3            0.1703             0.5401           0.5390   \n",
       "9       711.4            0.1853             1.0580           1.1050   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "5                0.1741          0.3985                  0.12440  \n",
       "6                0.1932          0.3063                  0.08368  \n",
       "7                0.1556          0.3196                  0.11510  \n",
       "8                0.2060          0.4378                  0.10720  \n",
       "9                0.2210          0.4366                  0.20750  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop('diagnosis',axis = 1)\n",
    "target = df['diagnosis']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-train shape:  (455, 30)\n",
      "x-test shape:  (114, 30)\n",
      "y-train shape:  (455,)\n",
      "y-test shape:  (114,)\n",
      "\n",
      "x-train shape:  (455, 30)\n",
      "x-test shape:  (114, 30)\n",
      "y-train shape:  (455, 1)\n",
      "y-test shape:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features,target, test_size = 0.2, random_state = 100)\n",
    "\n",
    "print('x-train shape: ', x_train.shape)\n",
    "print('x-test shape: ', x_test.shape)\n",
    "print('y-train shape: ', y_train.shape)\n",
    "print('y-test shape: ', y_test.shape)\n",
    "\n",
    "y_train = y_train.values.reshape(455,1)\n",
    "y_test = y_test.values.reshape(y_test.shape[0],1)\n",
    "\n",
    "print('\\nx-train shape: ', x_train.shape)\n",
    "print('x-test shape: ', x_test.shape)\n",
    "print('y-train shape: ', y_train.shape)\n",
    "print('y-test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to transform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "radius_mean                0.164102\n",
      "texture_mean               0.038848\n",
      "perimeter_mean             0.171231\n",
      "area_mean                  0.177140\n",
      "smoothness_mean            0.074547\n",
      "compactness_mean           0.124861\n",
      "concavity_mean             0.202130\n",
      "concave points_mean        0.143745\n",
      "symmetry_mean              0.166346\n",
      "fractal_dimension_mean     0.022857\n",
      "radius_se                  0.137223\n",
      "texture_se                -0.110760\n",
      "perimeter_se               0.140633\n",
      "area_se                    0.196212\n",
      "smoothness_se             -0.114964\n",
      "compactness_se            -0.000480\n",
      "concavity_se               0.124784\n",
      "concave points_se         -0.049870\n",
      "symmetry_se               -0.062455\n",
      "fractal_dimension_se      -0.013261\n",
      "radius_worst               0.216077\n",
      "texture_worst              0.085301\n",
      "perimeter_worst            0.226844\n",
      "area_worst                 0.242190\n",
      "smoothness_worst           0.146782\n",
      "compactness_worst          0.151012\n",
      "concavity_worst            0.237238\n",
      "concave points_worst       0.141887\n",
      "symmetry_worst             0.236775\n",
      "fractal_dimension_worst    0.123304\n",
      "dtype: float64\n",
      "test std = \n",
      "radius_mean                1.085337\n",
      "texture_mean               1.024918\n",
      "perimeter_mean             1.104167\n",
      "area_mean                  1.142946\n",
      "smoothness_mean            1.157307\n",
      "compactness_mean           1.214517\n",
      "concavity_mean             1.284181\n",
      "concave points_mean        1.184568\n",
      "symmetry_mean              1.224213\n",
      "fractal_dimension_mean     1.222073\n",
      "radius_se                  1.290508\n",
      "texture_se                 0.904528\n",
      "perimeter_se               1.316660\n",
      "area_se                    1.507379\n",
      "smoothness_se              0.935201\n",
      "compactness_se             0.998671\n",
      "concavity_se               1.516696\n",
      "concave points_se          1.167189\n",
      "symmetry_se                1.112518\n",
      "fractal_dimension_se       1.238989\n",
      "radius_worst               1.154716\n",
      "texture_worst              1.062852\n",
      "perimeter_worst            1.177430\n",
      "area_worst                 1.281922\n",
      "smoothness_worst           1.208620\n",
      "compactness_worst          1.135799\n",
      "concavity_worst            1.195304\n",
      "concave points_worst       1.125378\n",
      "symmetry_worst             1.314073\n",
      "fractal_dimension_worst    1.204697\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = np.mean(x_train, axis=0).values.reshape(1, d)\n",
    "sig = np.std(x_train, axis=0).values.reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(np.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(np.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 31) (114, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.2791425 ,  0.01254363, -0.36045282, ..., -0.75523988,\n",
       "        -1.19503909,  1.        ],\n",
       "       [-0.32838033,  2.26039875, -0.36509256, ..., -0.87170718,\n",
       "        -0.64297538,  1.        ],\n",
       "       [ 0.50287002, -0.00850808,  0.67252254, ...,  2.38416213,\n",
       "         1.24435261,  1.        ],\n",
       "       ...,\n",
       "       [-0.68173414, -0.51141011, -0.73627195, ..., -0.24765107,\n",
       "        -0.88453946,  1.        ],\n",
       "       [-0.33417302, -0.29387574, -0.33598873, ..., -0.16421182,\n",
       "        -0.25137509,  1.        ],\n",
       "       [-1.36671918, -1.25289822, -1.31581795, ...,  1.40375087,\n",
       "         0.36498985,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n ,d = x_train.shape         #ADD A VECTOR OF 1's\n",
    "if d == 30:\n",
    "    x_train = np.concatenate((x_train, np.ones((n,1))), axis = 1)\n",
    "\n",
    "n, d = x_test.shape\n",
    "if d == 30:\n",
    "    x_test = np.concatenate((x_test, np.ones((n,1))), axis = 1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(w, x, y, lam):\n",
    "    n,d = x.shape\n",
    "    yx = np.multiply(y,x)\n",
    "    yxw = np.dot(yx,w)\n",
    "    f1 = np.exp(-yxw)\n",
    "    f2 = np.log(1 + f1)\n",
    "    loss = np.mean(f2)\n",
    "    reg = lam / 2 * np.sum(w*w)\n",
    "    return loss + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital Value :  0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "d = x_train.shape[1]  # d = 31\n",
    "w = np.zeros((d,1))   # w = 31 -> 0 rows one column\n",
    "\n",
    "lam = 1E-6\n",
    "res_obj = objective(w , x_train , y_train , lam)\n",
    "print(\"Inital Value : \", res_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = np.multiply(y , x)\n",
    "    yxw = np.dot(yx , w)\n",
    "    f1 = np.exp(yxw)\n",
    "    f2 = np.divide(yx , 1+f1)\n",
    "    f3 = -np.mean(f2 , axis = 0).reshape(d , 1)\n",
    "    grad = f3 + (lam * w)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative processes (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n, d = x.shape\n",
    "    objvals = np.zeros(max_epoch) # OUTPUT\n",
    "    \n",
    "    w = np.zeros((d, 1)) \n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[i] = objval\n",
    "        print('Objective value at i = ' + str(i) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= learning_rate * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at i = 0 is 0.6931471805599453\n",
      "Objective value at i = 1 is 0.526098137148361\n",
      "Objective value at i = 2 is 0.43936470428149116\n",
      "Objective value at i = 3 is 0.38595701340246036\n",
      "Objective value at i = 4 is 0.3490810053292459\n",
      "Objective value at i = 5 is 0.3217088221233676\n",
      "Objective value at i = 6 is 0.30036595564633406\n",
      "Objective value at i = 7 is 0.2831217991137448\n",
      "Objective value at i = 8 is 0.2688100013292935\n",
      "Objective value at i = 9 is 0.25668032404378094\n",
      "Objective value at i = 10 is 0.24622604322881503\n",
      "Objective value at i = 11 is 0.23709124359292577\n",
      "Objective value at i = 12 is 0.2290177715113436\n",
      "Objective value at i = 13 is 0.22181328173935003\n",
      "Objective value at i = 14 is 0.21533115351484258\n",
      "Objective value at i = 15 is 0.20945740589009243\n",
      "Objective value at i = 16 is 0.20410190732954672\n",
      "Objective value at i = 17 is 0.1991923111442655\n",
      "Objective value at i = 18 is 0.1946697730389005\n",
      "Objective value at i = 19 is 0.19048586434237544\n",
      "Objective value at i = 20 is 0.18660030605319977\n",
      "Objective value at i = 21 is 0.18297927798034955\n",
      "Objective value at i = 22 is 0.17959413826790868\n",
      "Objective value at i = 23 is 0.1764204406483933\n",
      "Objective value at i = 24 is 0.17343717095972985\n",
      "Objective value at i = 25 is 0.17062614736389808\n",
      "Objective value at i = 26 is 0.16797154432527725\n",
      "Objective value at i = 27 is 0.1654595112358665\n",
      "Objective value at i = 28 is 0.1630778641960555\n",
      "Objective value at i = 29 is 0.16081583489846213\n",
      "Objective value at i = 30 is 0.15866386449362915\n",
      "Objective value at i = 31 is 0.15661343319189244\n",
      "Objective value at i = 32 is 0.15465691848236243\n",
      "Objective value at i = 33 is 0.15278747643900262\n",
      "Objective value at i = 34 is 0.15099894178260975\n",
      "Objective value at i = 35 is 0.14928574328010533\n",
      "Objective value at i = 36 is 0.14764283176317333\n",
      "Objective value at i = 37 is 0.14606561859045375\n",
      "Objective value at i = 38 is 0.14454992280022008\n",
      "Objective value at i = 39 is 0.1430919255323911\n",
      "Objective value at i = 40 is 0.14168813056110904\n",
      "Objective value at i = 41 is 0.14033532998784382\n",
      "Objective value at i = 42 is 0.1390305743120365\n",
      "Objective value at i = 43 is 0.13777114623075856\n",
      "Objective value at i = 44 is 0.13655453762768888\n",
      "Objective value at i = 45 is 0.13537842930024488\n",
      "Objective value at i = 46 is 0.13424067304608753\n",
      "Objective value at i = 47 is 0.1331392757896825\n",
      "Objective value at i = 48 is 0.1320723854786654\n",
      "Objective value at i = 49 is 0.1310382785204276\n",
      "Objective value at i = 50 is 0.13003534856318072\n",
      "Objective value at i = 51 is 0.12906209645403918\n",
      "Objective value at i = 52 is 0.12811712123037722\n",
      "Objective value at i = 53 is 0.12719911202068654\n",
      "Objective value at i = 54 is 0.12630684074802842\n",
      "Objective value at i = 55 is 0.1254391555434754\n",
      "Objective value at i = 56 is 0.12459497478909913\n",
      "Objective value at i = 57 is 0.12377328172043625\n",
      "Objective value at i = 58 is 0.12297311952724327\n",
      "Objective value at i = 59 is 0.12219358689897092\n",
      "Objective value at i = 60 is 0.12143383396794726\n",
      "Objective value at i = 61 is 0.12069305860891748\n",
      "Objective value at i = 62 is 0.11997050305848511\n",
      "Objective value at i = 63 is 0.11926545082224695\n",
      "Objective value at i = 64 is 0.11857722384110744\n",
      "Objective value at i = 65 is 0.1179051798914773\n",
      "Objective value at i = 66 is 0.11724871019687196\n",
      "Objective value at i = 67 is 0.11660723723088989\n",
      "Objective value at i = 68 is 0.11598021269370737\n",
      "Objective value at i = 69 is 0.11536711564612834\n",
      "Objective value at i = 70 is 0.11476745078689853\n",
      "Objective value at i = 71 is 0.11418074686047044\n",
      "Objective value at i = 72 is 0.11360655518371109\n",
      "Objective value at i = 73 is 0.11304444828120057\n",
      "Objective value at i = 74 is 0.11249401861979651\n",
      "Objective value at i = 75 is 0.1119548774340508\n",
      "Objective value at i = 76 is 0.11142665363487807\n",
      "Objective value at i = 77 is 0.11090899279459954\n",
      "Objective value at i = 78 is 0.11040155620213268\n",
      "Objective value at i = 79 is 0.10990401998267667\n",
      "Objective value at i = 80 is 0.10941607427676128\n",
      "Objective value at i = 81 is 0.10893742247399295\n",
      "Objective value at i = 82 is 0.10846778049724921\n",
      "Objective value at i = 83 is 0.10800687613344911\n",
      "Objective value at i = 84 is 0.10755444840736557\n",
      "Objective value at i = 85 is 0.10711024699525164\n",
      "Objective value at i = 86 is 0.10667403167532823\n",
      "Objective value at i = 87 is 0.10624557181243062\n",
      "Objective value at i = 88 is 0.10582464587433678\n",
      "Objective value at i = 89 is 0.10541104097750516\n",
      "Objective value at i = 90 is 0.10500455246013603\n",
      "Objective value at i = 91 is 0.10460498348063814\n",
      "Objective value at i = 92 is 0.10421214463973685\n",
      "Objective value at i = 93 is 0.10382585362459926\n",
      "Objective value at i = 94 is 0.1034459348734786\n",
      "Objective value at i = 95 is 0.10307221925949757\n",
      "Objective value at i = 96 is 0.10270454379229482\n",
      "Objective value at i = 97 is 0.10234275133635676\n",
      "Objective value at i = 98 is 0.10198669034494498\n",
      "Objective value at i = 99 is 0.101636214608611\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 1E-6\n",
    "learning_rate = 0.1\n",
    "w, res_gd = gradient_descent(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at i = 0 is 0.6931471805599453\n",
      "Objective value at i = 1 is 0.535944995108907\n",
      "Objective value at i = 2 is 0.47565241785207024\n",
      "Objective value at i = 3 is 0.4477821792914088\n",
      "Objective value at i = 4 is 0.4333292031035285\n",
      "Objective value at i = 5 is 0.4252919752553773\n",
      "Objective value at i = 6 is 0.4206072174284883\n",
      "Objective value at i = 7 is 0.4177808942460204\n",
      "Objective value at i = 8 is 0.41602933290882216\n",
      "Objective value at i = 9 is 0.4149196357624026\n",
      "Objective value at i = 10 is 0.41420323259061775\n",
      "Objective value at i = 11 is 0.4137330121063032\n",
      "Objective value at i = 12 is 0.413419746110471\n",
      "Objective value at i = 13 is 0.41320818718914265\n",
      "Objective value at i = 14 is 0.4130635116278064\n",
      "Objective value at i = 15 is 0.4129634181384801\n",
      "Objective value at i = 16 is 0.4128934175355776\n",
      "Objective value at i = 17 is 0.4128439700270669\n",
      "Objective value at i = 18 is 0.4128087158850413\n",
      "Objective value at i = 19 is 0.4127833653605017\n",
      "Objective value at i = 20 is 0.4127649926357491\n",
      "Objective value at i = 21 is 0.4127515810428136\n",
      "Objective value at i = 22 is 0.41274172662850805\n",
      "Objective value at i = 23 is 0.4127344427413996\n",
      "Objective value at i = 24 is 0.4127290298158688\n",
      "Objective value at i = 25 is 0.4127249876961115\n",
      "Objective value at i = 26 is 0.41272195600969586\n",
      "Objective value at i = 27 is 0.4127196732244873\n",
      "Objective value at i = 28 is 0.4127179482729977\n",
      "Objective value at i = 29 is 0.41271664071110964\n",
      "Objective value at i = 30 is 0.4127156467261372\n",
      "Objective value at i = 31 is 0.41271488918991844\n",
      "Objective value at i = 32 is 0.4127143105334202\n",
      "Objective value at i = 33 is 0.41271386760579265\n",
      "Objective value at i = 34 is 0.4127135279402453\n",
      "Objective value at i = 35 is 0.41271326702479916\n",
      "Objective value at i = 36 is 0.41271306629594673\n",
      "Objective value at i = 37 is 0.4127129116558735\n",
      "Objective value at i = 38 is 0.41271279237125463\n",
      "Objective value at i = 39 is 0.41271270025177653\n",
      "Objective value at i = 40 is 0.4127126290348327\n",
      "Objective value at i = 41 is 0.4127125739229417\n",
      "Objective value at i = 42 is 0.4127125312348145\n",
      "Objective value at i = 43 is 0.4127124981413485\n",
      "Objective value at i = 44 is 0.4127124724653298\n",
      "Objective value at i = 45 is 0.41271245252909106\n",
      "Objective value at i = 46 is 0.41271243703838323\n",
      "Objective value at i = 47 is 0.4127124249936729\n",
      "Objective value at i = 48 is 0.41271241562226163\n",
      "Objective value at i = 49 is 0.4127124083262538\n",
      "Objective value at i = 50 is 0.41271240264261067\n",
      "Objective value at i = 51 is 0.41271239821244216\n",
      "Objective value at i = 52 is 0.4127123947573671\n",
      "Objective value at i = 53 is 0.4127123920612969\n",
      "Objective value at i = 54 is 0.4127123899563804\n",
      "Objective value at i = 55 is 0.41271238831214985\n",
      "Objective value at i = 56 is 0.41271238702713153\n",
      "Objective value at i = 57 is 0.4127123860223536\n",
      "Objective value at i = 58 is 0.4127123852363212\n",
      "Objective value at i = 59 is 0.4127123846211208\n",
      "Objective value at i = 60 is 0.41271238413940103\n",
      "Objective value at i = 61 is 0.41271238376202785\n",
      "Objective value at i = 62 is 0.4127123834662656\n",
      "Objective value at i = 63 is 0.41271238323436243\n",
      "Objective value at i = 64 is 0.4127123830524511\n",
      "Objective value at i = 65 is 0.4127123829096935\n",
      "Objective value at i = 66 is 0.41271238279761485\n",
      "Objective value at i = 67 is 0.4127123827095855\n",
      "Objective value at i = 68 is 0.4127123826404165\n",
      "Objective value at i = 69 is 0.41271238258604503\n",
      "Objective value at i = 70 is 0.4127123825432882\n",
      "Objective value at i = 71 is 0.4127123825096516\n",
      "Objective value at i = 72 is 0.4127123824831797\n",
      "Objective value at i = 73 is 0.4127123824623382\n",
      "Objective value at i = 74 is 0.41271238244592323\n",
      "Objective value at i = 75 is 0.41271238243299\n",
      "Objective value at i = 76 is 0.4127123824227962\n",
      "Objective value at i = 77 is 0.41271238241475844\n",
      "Objective value at i = 78 is 0.4127123824084186\n",
      "Objective value at i = 79 is 0.41271238240341623\n",
      "Objective value at i = 80 is 0.4127123823994677\n",
      "Objective value at i = 81 is 0.41271238239634994\n",
      "Objective value at i = 82 is 0.4127123823938873\n",
      "Objective value at i = 83 is 0.4127123823919416\n",
      "Objective value at i = 84 is 0.41271238239040364\n",
      "Objective value at i = 85 is 0.4127123823891876\n",
      "Objective value at i = 86 is 0.41271238238822594\n",
      "Objective value at i = 87 is 0.41271238238746494\n",
      "Objective value at i = 88 is 0.41271238238686286\n",
      "Objective value at i = 89 is 0.4127123823863861\n",
      "Objective value at i = 90 is 0.41271238238600855\n",
      "Objective value at i = 91 is 0.41271238238570945\n",
      "Objective value at i = 92 is 0.41271238238547253\n",
      "Objective value at i = 93 is 0.41271238238528474\n",
      "Objective value at i = 94 is 0.41271238238513586\n",
      "Objective value at i = 95 is 0.4127123823850177\n",
      "Objective value at i = 96 is 0.412712382384924\n",
      "Objective value at i = 97 is 0.4127123823848496\n",
      "Objective value at i = 98 is 0.4127123823847907\n",
      "Objective value at i = 99 is 0.41271238238474384\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 1\n",
    "learning_rate = 0.1\n",
    "w, res_gd_reg = gradient_descent(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi \n",
    "    yxw = float(np.dot(yx, w)) \n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = np.log(1 + np.exp(-yxw)) \n",
    "    reg = lam / 2 * np.sum(w * w) \n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + np.exp(yxw)) \n",
    "    g = g_loss + lam * w \n",
    "    g = g[0].reshape((d,1))\n",
    "    return obj, g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n, d = x.shape\n",
    "    objvals = np.zeros(max_epoch) # OUTPUT\n",
    "    \n",
    "    w = np.zeros((d, 1)) # zeros\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly samples\n",
    "        random_indices = np.random.permutation(n)\n",
    "        x_random = x[random_indices, :]\n",
    "        y_random = y[random_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_random[i, :] \n",
    "            yi = float(y_random[i, :]) \n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            \n",
    "            w -= learning_rate * g\n",
    "        \n",
    "        learning_rate *= 0.5\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.10919243436153855\n",
      "Objective value at epoch t=1 is 0.06661029850396386\n",
      "Objective value at epoch t=2 is 0.061209198930385345\n",
      "Objective value at epoch t=3 is 0.058897050528169015\n",
      "Objective value at epoch t=4 is 0.057701702257155436\n",
      "Objective value at epoch t=5 is 0.0572145327482306\n",
      "Objective value at epoch t=6 is 0.05695643725902572\n",
      "Objective value at epoch t=7 is 0.05682859124894552\n",
      "Objective value at epoch t=8 is 0.05676492777526518\n",
      "Objective value at epoch t=9 is 0.05673312571996832\n",
      "Objective value at epoch t=10 is 0.05671723698105104\n",
      "Objective value at epoch t=11 is 0.056709294585790986\n",
      "Objective value at epoch t=12 is 0.05670532526750624\n",
      "Objective value at epoch t=13 is 0.05670334011536014\n",
      "Objective value at epoch t=14 is 0.05670234755491249\n",
      "Objective value at epoch t=15 is 0.05670185134742496\n",
      "Objective value at epoch t=16 is 0.056701603235274636\n",
      "Objective value at epoch t=17 is 0.056701479175784573\n",
      "Objective value at epoch t=18 is 0.056701417144850406\n",
      "Objective value at epoch t=19 is 0.05670138612970976\n",
      "Objective value at epoch t=20 is 0.056701370622781876\n",
      "Objective value at epoch t=21 is 0.05670136286903014\n",
      "Objective value at epoch t=22 is 0.05670135899233929\n",
      "Objective value at epoch t=23 is 0.056701357053983464\n",
      "Objective value at epoch t=24 is 0.05670135608478025\n",
      "Objective value at epoch t=25 is 0.05670135560015898\n",
      "Objective value at epoch t=26 is 0.05670135535786404\n",
      "Objective value at epoch t=27 is 0.056701355236716204\n",
      "Objective value at epoch t=28 is 0.05670135517614039\n",
      "Objective value at epoch t=29 is 0.05670135514585316\n",
      "Objective value at epoch t=30 is 0.056701355130709334\n",
      "Objective value at epoch t=31 is 0.056701355123137245\n",
      "Objective value at epoch t=32 is 0.0567013551193512\n",
      "Objective value at epoch t=33 is 0.056701355117458295\n",
      "Objective value at epoch t=34 is 0.05670135511651177\n",
      "Objective value at epoch t=35 is 0.05670135511603856\n",
      "Objective value at epoch t=36 is 0.056701355115801905\n",
      "Objective value at epoch t=37 is 0.056701355115683624\n",
      "Objective value at epoch t=38 is 0.05670135511562442\n",
      "Objective value at epoch t=39 is 0.05670135511559495\n",
      "Objective value at epoch t=40 is 0.056701355115580145\n",
      "Objective value at epoch t=41 is 0.05670135511557286\n",
      "Objective value at epoch t=42 is 0.056701355115569146\n",
      "Objective value at epoch t=43 is 0.05670135511556736\n",
      "Objective value at epoch t=44 is 0.05670135511556644\n",
      "Objective value at epoch t=45 is 0.05670135511556597\n",
      "Objective value at epoch t=46 is 0.05670135511556578\n",
      "Objective value at epoch t=47 is 0.05670135511556566\n",
      "Objective value at epoch t=48 is 0.05670135511556565\n",
      "Objective value at epoch t=49 is 0.05670135511556563\n",
      "Objective value at epoch t=50 is 0.05670135511556559\n",
      "Objective value at epoch t=51 is 0.05670135511556556\n",
      "Objective value at epoch t=52 is 0.056701355115565615\n",
      "Objective value at epoch t=53 is 0.05670135511556563\n",
      "Objective value at epoch t=54 is 0.056701355115565615\n",
      "Objective value at epoch t=55 is 0.056701355115565594\n",
      "Objective value at epoch t=56 is 0.05670135511556563\n",
      "Objective value at epoch t=57 is 0.05670135511556566\n",
      "Objective value at epoch t=58 is 0.05670135511556562\n",
      "Objective value at epoch t=59 is 0.05670135511556557\n",
      "Objective value at epoch t=60 is 0.05670135511556562\n",
      "Objective value at epoch t=61 is 0.056701355115565594\n",
      "Objective value at epoch t=62 is 0.056701355115565615\n",
      "Objective value at epoch t=63 is 0.056701355115565635\n",
      "Objective value at epoch t=64 is 0.05670135511556556\n",
      "Objective value at epoch t=65 is 0.056701355115565594\n",
      "Objective value at epoch t=66 is 0.056701355115565615\n",
      "Objective value at epoch t=67 is 0.056701355115565615\n",
      "Objective value at epoch t=68 is 0.056701355115565615\n",
      "Objective value at epoch t=69 is 0.05670135511556558\n",
      "Objective value at epoch t=70 is 0.05670135511556559\n",
      "Objective value at epoch t=71 is 0.056701355115565594\n",
      "Objective value at epoch t=72 is 0.056701355115565566\n",
      "Objective value at epoch t=73 is 0.05670135511556552\n",
      "Objective value at epoch t=74 is 0.056701355115565615\n",
      "Objective value at epoch t=75 is 0.056701355115565615\n",
      "Objective value at epoch t=76 is 0.05670135511556559\n",
      "Objective value at epoch t=77 is 0.0567013551155656\n",
      "Objective value at epoch t=78 is 0.056701355115565635\n",
      "Objective value at epoch t=79 is 0.05670135511556559\n",
      "Objective value at epoch t=80 is 0.05670135511556554\n",
      "Objective value at epoch t=81 is 0.056701355115565594\n",
      "Objective value at epoch t=82 is 0.05670135511556557\n",
      "Objective value at epoch t=83 is 0.05670135511556559\n",
      "Objective value at epoch t=84 is 0.0567013551155656\n",
      "Objective value at epoch t=85 is 0.056701355115565594\n",
      "Objective value at epoch t=86 is 0.056701355115565615\n",
      "Objective value at epoch t=87 is 0.05670135511556554\n",
      "Objective value at epoch t=88 is 0.056701355115565615\n",
      "Objective value at epoch t=89 is 0.0567013551155656\n",
      "Objective value at epoch t=90 is 0.05670135511556554\n",
      "Objective value at epoch t=91 is 0.056701355115565635\n",
      "Objective value at epoch t=92 is 0.05670135511556556\n",
      "Objective value at epoch t=93 is 0.056701355115565566\n",
      "Objective value at epoch t=94 is 0.0567013551155656\n",
      "Objective value at epoch t=95 is 0.056701355115565594\n",
      "Objective value at epoch t=96 is 0.05670135511556558\n",
      "Objective value at epoch t=97 is 0.056701355115565635\n",
      "Objective value at epoch t=98 is 0.05670135511556558\n",
      "Objective value at epoch t=99 is 0.05670135511556558\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "\n",
    "lam = 1E-6\n",
    "learning_rate = 0.1\n",
    "w, res_sgd = sgd(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.2667586839918997\n",
      "Objective value at epoch t=1 is 0.21401632234024143\n",
      "Objective value at epoch t=2 is 0.21659225113121672\n",
      "Objective value at epoch t=3 is 0.21801963136474425\n",
      "Objective value at epoch t=4 is 0.21932417884451802\n",
      "Objective value at epoch t=5 is 0.21998996960058226\n",
      "Objective value at epoch t=6 is 0.22013161669770492\n",
      "Objective value at epoch t=7 is 0.22023184056310938\n",
      "Objective value at epoch t=8 is 0.22031750202470185\n",
      "Objective value at epoch t=9 is 0.22035887417673183\n",
      "Objective value at epoch t=10 is 0.22037733929194497\n",
      "Objective value at epoch t=11 is 0.22038469239717345\n",
      "Objective value at epoch t=12 is 0.22039009990666167\n",
      "Objective value at epoch t=13 is 0.22039191844851377\n",
      "Objective value at epoch t=14 is 0.22039315307407734\n",
      "Objective value at epoch t=15 is 0.22039360418114354\n",
      "Objective value at epoch t=16 is 0.22039386415832893\n",
      "Objective value at epoch t=17 is 0.22039405560462189\n",
      "Objective value at epoch t=18 is 0.22039411825004063\n",
      "Objective value at epoch t=19 is 0.22039413873815034\n",
      "Objective value at epoch t=20 is 0.22039415667968612\n",
      "Objective value at epoch t=21 is 0.22039416554199656\n",
      "Objective value at epoch t=22 is 0.2203941690626748\n",
      "Objective value at epoch t=23 is 0.2203941722568226\n",
      "Objective value at epoch t=24 is 0.2203941731418604\n",
      "Objective value at epoch t=25 is 0.2203941736320191\n",
      "Objective value at epoch t=26 is 0.2203941740035051\n",
      "Objective value at epoch t=27 is 0.22039417410749615\n",
      "Objective value at epoch t=28 is 0.22039417416035118\n",
      "Objective value at epoch t=29 is 0.22039417419902224\n",
      "Objective value at epoch t=30 is 0.22039417422007684\n",
      "Objective value at epoch t=31 is 0.22039417422661162\n",
      "Objective value at epoch t=32 is 0.22039417423181437\n",
      "Objective value at epoch t=33 is 0.22039417423412092\n",
      "Objective value at epoch t=34 is 0.22039417423480404\n",
      "Objective value at epoch t=35 is 0.22039417423538682\n",
      "Objective value at epoch t=36 is 0.2203941742356254\n",
      "Objective value at epoch t=37 is 0.2203941742357891\n",
      "Objective value at epoch t=38 is 0.22039417423583416\n",
      "Objective value at epoch t=39 is 0.22039417423586147\n",
      "Objective value at epoch t=40 is 0.22039417423587218\n",
      "Objective value at epoch t=41 is 0.2203941742358822\n",
      "Objective value at epoch t=42 is 0.22039417423588736\n",
      "Objective value at epoch t=43 is 0.22039417423588892\n",
      "Objective value at epoch t=44 is 0.22039417423589022\n",
      "Objective value at epoch t=45 is 0.22039417423589058\n",
      "Objective value at epoch t=46 is 0.22039417423589036\n",
      "Objective value at epoch t=47 is 0.2203941742358903\n",
      "Objective value at epoch t=48 is 0.2203941742358904\n",
      "Objective value at epoch t=49 is 0.22039417423589025\n",
      "Objective value at epoch t=50 is 0.22039417423589036\n",
      "Objective value at epoch t=51 is 0.22039417423589008\n",
      "Objective value at epoch t=52 is 0.22039417423589022\n",
      "Objective value at epoch t=53 is 0.22039417423589008\n",
      "Objective value at epoch t=54 is 0.22039417423589025\n",
      "Objective value at epoch t=55 is 0.22039417423589022\n",
      "Objective value at epoch t=56 is 0.22039417423589025\n",
      "Objective value at epoch t=57 is 0.22039417423589047\n",
      "Objective value at epoch t=58 is 0.2203941742358901\n",
      "Objective value at epoch t=59 is 0.22039417423589014\n",
      "Objective value at epoch t=60 is 0.2203941742358902\n",
      "Objective value at epoch t=61 is 0.2203941742358903\n",
      "Objective value at epoch t=62 is 0.2203941742358902\n",
      "Objective value at epoch t=63 is 0.2203941742358903\n",
      "Objective value at epoch t=64 is 0.22039417423589022\n",
      "Objective value at epoch t=65 is 0.22039417423589036\n",
      "Objective value at epoch t=66 is 0.2203941742358902\n",
      "Objective value at epoch t=67 is 0.22039417423589033\n",
      "Objective value at epoch t=68 is 0.22039417423589017\n",
      "Objective value at epoch t=69 is 0.2203941742358901\n",
      "Objective value at epoch t=70 is 0.2203941742358903\n",
      "Objective value at epoch t=71 is 0.22039417423589022\n",
      "Objective value at epoch t=72 is 0.22039417423589\n",
      "Objective value at epoch t=73 is 0.22039417423589025\n",
      "Objective value at epoch t=74 is 0.22039417423589014\n",
      "Objective value at epoch t=75 is 0.2203941742358903\n",
      "Objective value at epoch t=76 is 0.22039417423589014\n",
      "Objective value at epoch t=77 is 0.22039417423589036\n",
      "Objective value at epoch t=78 is 0.22039417423588983\n",
      "Objective value at epoch t=79 is 0.22039417423589014\n",
      "Objective value at epoch t=80 is 0.2203941742358903\n",
      "Objective value at epoch t=81 is 0.2203941742358901\n",
      "Objective value at epoch t=82 is 0.2203941742358904\n",
      "Objective value at epoch t=83 is 0.2203941742358903\n",
      "Objective value at epoch t=84 is 0.22039417423589033\n",
      "Objective value at epoch t=85 is 0.22039417423589025\n",
      "Objective value at epoch t=86 is 0.22039417423589022\n",
      "Objective value at epoch t=87 is 0.22039417423589022\n",
      "Objective value at epoch t=88 is 0.22039417423589017\n",
      "Objective value at epoch t=89 is 0.2203941742358902\n",
      "Objective value at epoch t=90 is 0.22039417423589006\n",
      "Objective value at epoch t=91 is 0.22039417423589017\n",
      "Objective value at epoch t=92 is 0.2203941742358904\n",
      "Objective value at epoch t=93 is 0.22039417423589044\n",
      "Objective value at epoch t=94 is 0.22039417423589044\n",
      "Objective value at epoch t=95 is 0.22039417423589025\n",
      "Objective value at epoch t=96 is 0.22039417423589033\n",
      "Objective value at epoch t=97 is 0.22039417423589017\n",
      "Objective value at epoch t=98 is 0.22039417423589017\n",
      "Objective value at epoch t=99 is 0.22039417423589025\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 0.1\n",
    "learning_rate = 0.01\n",
    "w, res_sgd_reg = sgd(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-b matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    n,d = xi.shape\n",
    "    b = xi.shape[0]\n",
    "    yx = numpy.multiply(yi, xi)\n",
    "    yxw = numpy.dot(yx, w) \n",
    "    \n",
    "    # objective function\n",
    "    loss = numpy.mean(numpy.log(1 + numpy.exp(-yxw))) \n",
    "    reg = lam/2 * numpy.sum(w * w) \n",
    "    obj = loss + reg\n",
    "    \n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -numpy.mean((numpy.divide(yx, 1 + numpy.exp(yxw))), axis=0).reshape(d, 1) \n",
    "    g = g_loss + lam * w \n",
    "    \n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n, d = x.shape\n",
    "    b = 10\n",
    "    objvals = numpy.zeros(max_epoch)\n",
    "    batch_nums = math.floor(n / b)\n",
    "    \n",
    "    w = numpy.zeros((d, 1))\n",
    "    \n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly samples\n",
    "        random_indices = numpy.random.permutation(n)\n",
    "        x_random = x[random_indices, :]\n",
    "        y_random = y[random_indices, :]\n",
    "        objval = 0 \n",
    "        \n",
    "        for i in range(0, n, b):\n",
    "            xi = x_random[i:i+b,:] \n",
    "            yi = y_random[i:i+b,:] \n",
    "            obj, g = mb_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= learning_rate * g\n",
    "        learning_rate *= 0.5\n",
    "        objval = objval/batch_nums\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))   \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.14021408536282726\n",
      "Objective value at epoch t=1 is 0.08152548274671002\n",
      "Objective value at epoch t=2 is 0.07530950549085237\n",
      "Objective value at epoch t=3 is 0.07211245055704489\n",
      "Objective value at epoch t=4 is 0.07242943293743404\n",
      "Objective value at epoch t=5 is 0.07100647552568601\n",
      "Objective value at epoch t=6 is 0.07027138488379091\n",
      "Objective value at epoch t=7 is 0.0707742843997839\n",
      "Objective value at epoch t=8 is 0.0716231504064727\n",
      "Objective value at epoch t=9 is 0.07053737063301242\n",
      "Objective value at epoch t=10 is 0.06978422003845484\n",
      "Objective value at epoch t=11 is 0.07049850614487439\n",
      "Objective value at epoch t=12 is 0.07004619858401402\n",
      "Objective value at epoch t=13 is 0.06982888496605276\n",
      "Objective value at epoch t=14 is 0.06996861436536823\n",
      "Objective value at epoch t=15 is 0.06989154164401838\n",
      "Objective value at epoch t=16 is 0.07070397668062538\n",
      "Objective value at epoch t=17 is 0.06993333264780476\n",
      "Objective value at epoch t=18 is 0.07171633156612545\n",
      "Objective value at epoch t=19 is 0.07061384754423944\n",
      "Objective value at epoch t=20 is 0.06986773529581315\n",
      "Objective value at epoch t=21 is 0.06991356781684174\n",
      "Objective value at epoch t=22 is 0.06974705978780377\n",
      "Objective value at epoch t=23 is 0.06970157952634409\n",
      "Objective value at epoch t=24 is 0.06989769712736975\n",
      "Objective value at epoch t=25 is 0.0698522191381007\n",
      "Objective value at epoch t=26 is 0.07055363090437011\n",
      "Objective value at epoch t=27 is 0.07073191045738832\n",
      "Objective value at epoch t=28 is 0.07021886642746186\n",
      "Objective value at epoch t=29 is 0.07005383327057543\n",
      "Objective value at epoch t=30 is 0.06998110214316798\n",
      "Objective value at epoch t=31 is 0.06973027826355996\n",
      "Objective value at epoch t=32 is 0.06979765351614478\n",
      "Objective value at epoch t=33 is 0.06995911725241738\n",
      "Objective value at epoch t=34 is 0.0713979358598325\n",
      "Objective value at epoch t=35 is 0.06975492026605361\n",
      "Objective value at epoch t=36 is 0.070413196212397\n",
      "Objective value at epoch t=37 is 0.06971640702047616\n",
      "Objective value at epoch t=38 is 0.06979911766315558\n",
      "Objective value at epoch t=39 is 0.07149189835620805\n",
      "Objective value at epoch t=40 is 0.07031189809641557\n",
      "Objective value at epoch t=41 is 0.0700615378716956\n",
      "Objective value at epoch t=42 is 0.07526570091042874\n",
      "Objective value at epoch t=43 is 0.07011787720821867\n",
      "Objective value at epoch t=44 is 0.0697325317531324\n",
      "Objective value at epoch t=45 is 0.07072524834436297\n",
      "Objective value at epoch t=46 is 0.0706786141321496\n",
      "Objective value at epoch t=47 is 0.07082805957556579\n",
      "Objective value at epoch t=48 is 0.07028197614261968\n",
      "Objective value at epoch t=49 is 0.07033974150474381\n",
      "Objective value at epoch t=50 is 0.06973479283935477\n",
      "Objective value at epoch t=51 is 0.0697853430445984\n",
      "Objective value at epoch t=52 is 0.07012021021185262\n",
      "Objective value at epoch t=53 is 0.0700560847549239\n",
      "Objective value at epoch t=54 is 0.06980140454823641\n",
      "Objective value at epoch t=55 is 0.0699188441912583\n",
      "Objective value at epoch t=56 is 0.06990740969409148\n",
      "Objective value at epoch t=57 is 0.07002150904255508\n",
      "Objective value at epoch t=58 is 0.07061123840088525\n",
      "Objective value at epoch t=59 is 0.06975943888249068\n",
      "Objective value at epoch t=60 is 0.07088664205976783\n",
      "Objective value at epoch t=61 is 0.06988797544192789\n",
      "Objective value at epoch t=62 is 0.06972094740819962\n",
      "Objective value at epoch t=63 is 0.07091606224129973\n",
      "Objective value at epoch t=64 is 0.08371805233989825\n",
      "Objective value at epoch t=65 is 0.06979928755230924\n",
      "Objective value at epoch t=66 is 0.06987962820759835\n",
      "Objective value at epoch t=67 is 0.0697759018963935\n",
      "Objective value at epoch t=68 is 0.070189712483979\n",
      "Objective value at epoch t=69 is 0.07138589145400415\n",
      "Objective value at epoch t=70 is 0.0698580718107945\n",
      "Objective value at epoch t=71 is 0.06974012478215563\n",
      "Objective value at epoch t=72 is 0.07080008704928714\n",
      "Objective value at epoch t=73 is 0.06987272501260373\n",
      "Objective value at epoch t=74 is 0.07000045008974654\n",
      "Objective value at epoch t=75 is 0.06987318940604714\n",
      "Objective value at epoch t=76 is 0.06968876202934442\n",
      "Objective value at epoch t=77 is 0.06978682167494511\n",
      "Objective value at epoch t=78 is 0.07469678825856269\n",
      "Objective value at epoch t=79 is 0.06992593900072212\n",
      "Objective value at epoch t=80 is 0.06973966281882918\n",
      "Objective value at epoch t=81 is 0.06980611071447472\n",
      "Objective value at epoch t=82 is 0.06993717183868403\n",
      "Objective value at epoch t=83 is 0.06993424767837685\n",
      "Objective value at epoch t=84 is 0.06978466708700468\n",
      "Objective value at epoch t=85 is 0.07183834579889183\n",
      "Objective value at epoch t=86 is 0.07048942542691473\n",
      "Objective value at epoch t=87 is 0.07123168406991326\n",
      "Objective value at epoch t=88 is 0.07003138475182591\n",
      "Objective value at epoch t=89 is 0.07112797122227131\n",
      "Objective value at epoch t=90 is 0.06987056989016162\n",
      "Objective value at epoch t=91 is 0.06990102134960519\n",
      "Objective value at epoch t=92 is 0.07047130430708458\n",
      "Objective value at epoch t=93 is 0.06994980423566953\n",
      "Objective value at epoch t=94 is 0.06996169796891077\n",
      "Objective value at epoch t=95 is 0.069855803163751\n",
      "Objective value at epoch t=96 is 0.06973433681132685\n",
      "Objective value at epoch t=97 is 0.07003669148321331\n",
      "Objective value at epoch t=98 is 0.07015391592642305\n",
      "Objective value at epoch t=99 is 0.06984813707067272\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 1E-6 \n",
    "\n",
    "learning_rate = 0.4\n",
    "\n",
    "w, res_mbgd = mbgd(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.45927074978378624\n",
      "Objective value at epoch t=1 is 0.43256662730663875\n",
      "Objective value at epoch t=2 is 0.42617322060143936\n",
      "Objective value at epoch t=3 is 0.4269608371867279\n",
      "Objective value at epoch t=4 is 0.42376244107203953\n",
      "Objective value at epoch t=5 is 0.4227326917846623\n",
      "Objective value at epoch t=6 is 0.42359034257573197\n",
      "Objective value at epoch t=7 is 0.420846981084792\n",
      "Objective value at epoch t=8 is 0.42256052462104216\n",
      "Objective value at epoch t=9 is 0.4206858839170025\n",
      "Objective value at epoch t=10 is 0.4259441964129321\n",
      "Objective value at epoch t=11 is 0.4234525527110131\n",
      "Objective value at epoch t=12 is 0.422843847899293\n",
      "Objective value at epoch t=13 is 0.4202311845529495\n",
      "Objective value at epoch t=14 is 0.42018905870164314\n",
      "Objective value at epoch t=15 is 0.42186533688304545\n",
      "Objective value at epoch t=16 is 0.4224466512954053\n",
      "Objective value at epoch t=17 is 0.4221328104714569\n",
      "Objective value at epoch t=18 is 0.4213526875890552\n",
      "Objective value at epoch t=19 is 0.421104107916964\n",
      "Objective value at epoch t=20 is 0.42329341005220744\n",
      "Objective value at epoch t=21 is 0.42181267593357935\n",
      "Objective value at epoch t=22 is 0.4210486284191414\n",
      "Objective value at epoch t=23 is 0.4213789441511597\n",
      "Objective value at epoch t=24 is 0.4227069494722588\n",
      "Objective value at epoch t=25 is 0.4217560122070156\n",
      "Objective value at epoch t=26 is 0.421613297856854\n",
      "Objective value at epoch t=27 is 0.4234406209088077\n",
      "Objective value at epoch t=28 is 0.42254614512549854\n",
      "Objective value at epoch t=29 is 0.42220160279567703\n",
      "Objective value at epoch t=30 is 0.42228539642662977\n",
      "Objective value at epoch t=31 is 0.4227888288782674\n",
      "Objective value at epoch t=32 is 0.42184024675272136\n",
      "Objective value at epoch t=33 is 0.42069460843093287\n",
      "Objective value at epoch t=34 is 0.42353347206958547\n",
      "Objective value at epoch t=35 is 0.4228833107907314\n",
      "Objective value at epoch t=36 is 0.4208586551876326\n",
      "Objective value at epoch t=37 is 0.4214899794374937\n",
      "Objective value at epoch t=38 is 0.42358140291999513\n",
      "Objective value at epoch t=39 is 0.42295982395354514\n",
      "Objective value at epoch t=40 is 0.4225107242723023\n",
      "Objective value at epoch t=41 is 0.4229188942830399\n",
      "Objective value at epoch t=42 is 0.42395243680449146\n",
      "Objective value at epoch t=43 is 0.4212985143330928\n",
      "Objective value at epoch t=44 is 0.42037603826423453\n",
      "Objective value at epoch t=45 is 0.42158868733925414\n",
      "Objective value at epoch t=46 is 0.4210104564332965\n",
      "Objective value at epoch t=47 is 0.4202700780327805\n",
      "Objective value at epoch t=48 is 0.421892736278532\n",
      "Objective value at epoch t=49 is 0.4242116839507448\n",
      "Objective value at epoch t=50 is 0.42121657547484886\n",
      "Objective value at epoch t=51 is 0.4208374799120647\n",
      "Objective value at epoch t=52 is 0.42191151791140746\n",
      "Objective value at epoch t=53 is 0.4227941724233147\n",
      "Objective value at epoch t=54 is 0.4223457905309111\n",
      "Objective value at epoch t=55 is 0.4210392943271518\n",
      "Objective value at epoch t=56 is 0.4205345529172666\n",
      "Objective value at epoch t=57 is 0.42450272947639317\n",
      "Objective value at epoch t=58 is 0.4213344421787356\n",
      "Objective value at epoch t=59 is 0.42165585872817807\n",
      "Objective value at epoch t=60 is 0.42197920101157493\n",
      "Objective value at epoch t=61 is 0.4206568256230109\n",
      "Objective value at epoch t=62 is 0.41944651837160457\n",
      "Objective value at epoch t=63 is 0.42066406312087934\n",
      "Objective value at epoch t=64 is 0.4212960370717266\n",
      "Objective value at epoch t=65 is 0.4228334655673435\n",
      "Objective value at epoch t=66 is 0.4213992565366552\n",
      "Objective value at epoch t=67 is 0.4224529034326935\n",
      "Objective value at epoch t=68 is 0.42100799964486096\n",
      "Objective value at epoch t=69 is 0.42115513547496874\n",
      "Objective value at epoch t=70 is 0.4203220539755467\n",
      "Objective value at epoch t=71 is 0.4220378289556499\n",
      "Objective value at epoch t=72 is 0.42353030789128404\n",
      "Objective value at epoch t=73 is 0.42059582898978365\n",
      "Objective value at epoch t=74 is 0.42114730707016085\n",
      "Objective value at epoch t=75 is 0.42168602540147615\n",
      "Objective value at epoch t=76 is 0.4234956599092934\n",
      "Objective value at epoch t=77 is 0.4209297252984491\n",
      "Objective value at epoch t=78 is 0.42189234976215784\n",
      "Objective value at epoch t=79 is 0.4210494144643059\n",
      "Objective value at epoch t=80 is 0.4233530173820181\n",
      "Objective value at epoch t=81 is 0.4211317070403214\n",
      "Objective value at epoch t=82 is 0.4211933156140755\n",
      "Objective value at epoch t=83 is 0.42330620396000374\n",
      "Objective value at epoch t=84 is 0.4216516075322319\n",
      "Objective value at epoch t=85 is 0.42209650987869923\n",
      "Objective value at epoch t=86 is 0.4210670277928133\n",
      "Objective value at epoch t=87 is 0.42061952575516914\n",
      "Objective value at epoch t=88 is 0.42116229965327795\n",
      "Objective value at epoch t=89 is 0.42152858787460934\n",
      "Objective value at epoch t=90 is 0.42180296345011575\n",
      "Objective value at epoch t=91 is 0.42067763658528606\n",
      "Objective value at epoch t=92 is 0.4211732094796489\n",
      "Objective value at epoch t=93 is 0.4224687506565014\n",
      "Objective value at epoch t=94 is 0.421642709442294\n",
      "Objective value at epoch t=95 is 0.4232096757493456\n",
      "Objective value at epoch t=96 is 0.4225284827836249\n",
      "Objective value at epoch t=97 is 0.4218616838013202\n",
      "Objective value at epoch t=98 is 0.42151250890724695\n",
      "Objective value at epoch t=99 is 0.422521628664872\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 1 \n",
    "stepsize = 0.4\n",
    "w, res_mbgd_reg = mbgd(x_train, y_train, lam, learning_rate, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs_gd = range(len(res_gd))\n",
    "epochs_sgd = range(len(res_sgd))\n",
    "epochs_mbgd = range(len(res_mbgd))\n",
    "\n",
    "\n",
    "l1, = plt.plot(epochs_gd, res_gd, '--r')\n",
    "l2, = plt.plot(epochs_sgd, res_sgd, 'bo')\n",
    "l3, = plt.plot(epochs_mbgd, res_mbgd, 'go')\n",
    "\n",
    "plt.xlabel('Epochs', fontsize = 10)\n",
    "plt.ylabel('Objective Value', fontsize = 10)\n",
    "\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "\n",
    "plt.legend([l1, l2, l3], ['GD', 'SGD', 'MBG'], fontsize = 10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.046153846153846156\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistic regression and regularized version\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistic regression and regularized version\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
